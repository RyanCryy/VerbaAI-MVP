[1mdiff --git a/app/api/transcribe/route.ts b/app/api/transcribe/route.ts[m
[1mindex 6f5cf9d..f10263d 100644[m
[1m--- a/app/api/transcribe/route.ts[m
[1m+++ b/app/api/transcribe/route.ts[m
[36m@@ -55,16 +55,23 @@[m [mexport async function POST(request: NextRequest) {[m
         model: 'latest_long',[m
         enableAutomaticPunctuation: true,[m
         useEnhanced: true,[m
[31m-        audioChannelCount: 2  // Specify 2 channels for stereo audio[m
[32m+[m[32m        audioChannelCount: 2,  // Specify 2 channels for stereo audio[m
[32m+[m[32m        enableSeparateRecognitionPerChannel: true,[m
[32m+[m[32m        enableWordTimeOffsets: true,[m
[32m+[m[32m        diarizationConfig: {[m
[32m+[m[32m          enableSpeakerDiarization: true,[m
[32m+[m[32m          minSpeakerCount: 2,[m
[32m+[m[32m          maxSpeakerCount: 2[m
[32m+[m[32m        }[m
       },[m
       audio: {[m
         content: audioContent[m
       }[m
     }[m
 [m
[31m-    // Call Google Cloud Speech-to-Text API[m
[32m+[m[32m    // Call Google Cloud Speech-to-Text API using v1p1beta1 endpoint for speaker diarization[m
     const response = await fetch([m
[31m-      `https://speech.googleapis.com/v1/speech:recognize?key=${process.env.GOOGLE_CLOUD_API_KEY}`,[m
[32m+[m[32m      `https://speech.googleapis.com/v1p1beta1/speech:recognize?key=${process.env.GOOGLE_CLOUD_API_KEY}`,[m
       {[m
         method: 'POST',[m
         headers: {[m
[36m@@ -81,17 +88,43 @@[m [mexport async function POST(request: NextRequest) {[m
 [m
     const data = await response.json()[m
 [m
[31m-    // Extract transcription from response[m
[31m-    const transcription = data.results[m
[31m-      ?.map((result: any) => result.alternatives?.[0]?.transcript)[m
[31m-      .join('\n')[m
[32m+[m[32m    // Process the diarized transcription[m
[32m+[m[32m    let currentSpeaker = -1[m
[32m+[m[32m    let conversationText = ''[m
[32m+[m[32m    let currentUtterance = ''[m
 [m
[31m-    if (!transcription) {[m
[31m-      throw new Error('No transcription returned from Google Speech-to-Text')[m
[32m+[m[32m    // Process each word with speaker tags[m
[32m+[m[32m    data.results?.forEach((result: any) => {[m
[32m+[m[32m      if (result.alternatives?.[0]?.words) {[m
[32m+[m[32m        const words = result.alternatives[0].words[m
[32m+[m
[32m+[m[32m        words.forEach((word: any) => {[m
[32m+[m[32m          const speakerTag = word.speakerTag || 0[m
[32m+[m
[32m+[m[32m          if (currentSpeaker !== speakerTag) {[m
[32m+[m[32m            // If we have accumulated text for the previous speaker, add it[m
[32m+[m[32m            if (currentUtterance.trim()) {[m
[32m+[m[32m              conversationText += `Speaker ${currentSpeaker + 1}: ${currentUtterance.trim()}\n\n`[m
[32m+[m[32m            }[m
[32m+[m
[32m+[m[32m            // Start new utterance for new speaker[m
[32m+[m[32m            currentSpeaker = speakerTag[m
[32m+[m[32m            currentUtterance = word.word + ' '[m
[32m+[m[32m          } else {[m
[32m+[m[32m            // Continue current utterance[m
[32m+[m[32m            currentUtterance += word.word + ' '[m
[32m+[m[32m          }[m
[32m+[m[32m        })[m
[32m+[m[32m      }[m
[32m+[m[32m    })[m
[32m+[m
[32m+[m[32m    // Add the last utterance[m
[32m+[m[32m    if (currentUtterance.trim()) {[m
[32m+[m[32m      conversationText += `Speaker ${currentSpeaker + 1}: ${currentUtterance.trim()}\n\n`[m
     }[m
 [m
[31m-    console.log("Transcription successful")[m
[31m-    return NextResponse.json({ text: transcription }, { headers: corsHeaders })[m
[32m+[m[32m    console.log("Transcription with diarization successful")[m
[32m+[m[32m    return NextResponse.json({ text: conversationText.trim() }, { headers: corsHeaders })[m
   } catch (error: any) {[m
     console.error("Error in transcription API:", error)[m
     return NextResponse.json([m
